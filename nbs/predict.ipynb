{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict",
      "provenance": [],
      "collapsed_sections": [
        "QE9MtVZcbSKA",
        "1YZlrZHZ0MMz",
        "BWjrGsEvz6wu",
        "syQPyNFaz-Jx",
        "yQcFhIG4dGQw",
        "erQbncE7cF5C"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EAxjl6jbvLf"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d6Ei2JWlIck"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from pytorch_transformers import BertModel, BertTokenizer, BertConfig, WarmupLinearSchedule \n",
        "import re\n",
        "import pandas as pd \n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import pickle\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3Gkme_D_PT1"
      },
      "source": [
        "from tqdm import tqdm_notebook, trange\n",
        "\n",
        "def seed_everything(seed = 42): \n",
        "  random.seed(seed) \n",
        "  os.environ['PYTHONHASHSEED'] = str(seed) \n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed) \n",
        "  torch.cuda.manual_seed(seed) \n",
        "  torch.backends.cudnn.deterministic = True\n",
        "# For reproducible results\n",
        "seed_everything()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJJi_2trzTPl"
      },
      "source": [
        "import matplotlib as mpl\n",
        "mpl.style.use('seaborn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtqzkQEJSjh6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqW8xxNnSrO_"
      },
      "source": [
        "%cd /content/gdrive/My Drive/seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9GQVgq9IDMM"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGU-lHKjbLgW"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP0h_UdxBrGt"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.data = X\n",
        "        self.target = Y\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE9MtVZcbSKA"
      },
      "source": [
        "### Data Encoding Type 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-4faQd3XThr"
      },
      "source": [
        "data, label = 'data/', 'data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saKXaVYvQcRz"
      },
      "source": [
        "with open(data,'rb') as fp:\n",
        "    X = pickle.load(fp)\n",
        "with open(label,'rb') as fp:\n",
        "    Y = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ped5CnW7mqi8"
      },
      "source": [
        "X, Y = shuffle(X, Y, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38q1_Fc72UkB"
      },
      "source": [
        "Counter(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTt2G4J2aZMs"
      },
      "source": [
        "train_X = torch.from_numpy(np.asarray(X[:42000], np.float32))\n",
        "train_y = torch.from_numpy(np.asarray(Y[:42000]))\n",
        "test_X = torch.from_numpy(np.asarray(X[42000:56000],np.float32))\n",
        "test_y = torch.from_numpy(np.asarray(Y[42000:56000]))\n",
        "valid_X = torch.from_numpy(np.asarray(X[56000:],np.float32))\n",
        "valid_y = torch.from_numpy(np.asarray(Y[56000:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlD6bmCMblOk"
      },
      "source": [
        "### Data Encoding Type 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiXIl3f3dIy-"
      },
      "source": [
        "npzfile = np.load('data/50cut/200000peaks-InhNeuron.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SnYJfVbzX-Y"
      },
      "source": [
        "npzfile.files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC512aflzdwJ"
      },
      "source": [
        "X, y = npzfile['arr_0'], npzfile['arr_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6hpXV59JSrV"
      },
      "source": [
        "classes = max(y) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7kUsa-bzpxV"
      },
      "source": [
        "subX, subY = shuffle(X, y, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu9BaboMjiV3"
      },
      "source": [
        "testX = subX[int(len(subY)*0.8):]\n",
        "testY = subY[int(len(subY)*0.8):]\n",
        "validX = subX[int(len(subY)*0.6):int(len(subY)*0.8)]\n",
        "validY = subY[int(len(subY)*0.6):int(len(subY)*0.8)]\n",
        "trainX = subX[:int(len(subY)*0.6)]\n",
        "trainY = subY[:int(len(subY)*0.6)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyi5PicYjWsY"
      },
      "source": [
        "Counter(subY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI_f7eb1WX-_"
      },
      "source": [
        "#### Normalized Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-Jc5l5zbISO"
      },
      "source": [
        "cap = 30000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEKyKGoSbgAD"
      },
      "source": [
        "normX, normY = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EynVRABhbZIB"
      },
      "source": [
        "for idx, y in enumerate(subY):\n",
        "  if y == 0.0 and cap > 0:\n",
        "    normY.append(y)\n",
        "    normX.append(subX[idx])\n",
        "    cap -= 1\n",
        "  else:\n",
        "    if y == 0.0: continue\n",
        "    normY.append(y)\n",
        "    normX.append(subX[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx2ag0vNdyCJ"
      },
      "source": [
        "for idx, y in enumerate(subY):\n",
        "  if y == 0 and cap > 0:\n",
        "    normY.append(y)\n",
        "    normX.append(subX[idx])\n",
        "    cap -= 1\n",
        "  else:\n",
        "    if y == 0: continue\n",
        "    normY.append(y)\n",
        "    normX.append(subX[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nI9arGEbxZZ"
      },
      "source": [
        "normX, normY = shuffle(normX, normY, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjgCd7x4cBMw"
      },
      "source": [
        "normX, normY = np.array(normX), np.array(normY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0CYK2XbWXWX"
      },
      "source": [
        "testX = normX[int(len(normY)*0.8):]\n",
        "testY = normY[int(len(normY)*0.8):]\n",
        "validX = normX[int(len(normY)*0.6):int(len(normY)*0.8)]\n",
        "validY = normY[int(len(normY)*0.6):int(len(normY)*0.8)]\n",
        "trainX = normX[:int(len(normY)*0.6)]\n",
        "trainY = normY[:int(len(normY)*0.6)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-opxfm3YAckO"
      },
      "source": [
        "### Convert to Torch Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytpARg9V7V7r"
      },
      "source": [
        "train_X = torch.from_numpy(trainX)\n",
        "train_y = torch.from_numpy(trainY)\n",
        "valid_X  = torch.from_numpy(validX)\n",
        "valid_y = torch.from_numpy(validY)\n",
        "test_X = torch.from_numpy(testX)\n",
        "test_y = torch.from_numpy(testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFH7pdK_g4YR"
      },
      "source": [
        "train_dataset = MyDataset(train_X, train_y)\n",
        "valid_dataset = MyDataset(valid_X, valid_y)\n",
        "test_dataset = MyDataset(test_X, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPm33TNhFkXt"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsgWM3CPFo9C"
      },
      "source": [
        "### Save Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1AHblA7FrcJ"
      },
      "source": [
        "def bestmodel(model_name,save_model_time,valid_loss):\n",
        "    bestloss = 10000\n",
        "    if valid_loss < bestloss :\n",
        "        bestloss = valid_loss\n",
        "        torch.save(model_name, 'model/model{save_model_time}/bestmodel.pkl'.format(save_model_time=save_model_time))\n",
        "        torch.save(model_name.state_dict(), 'model/model{save_model_time}/net_params_bestmodel.pkl'.format(save_model_time=save_model_time))\n",
        "    return True  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOO1O-QgwMN3"
      },
      "source": [
        "### One Hot Encoding y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UDpkn1mwKUv"
      },
      "source": [
        "def onehot(y):\n",
        "    y_onehot = np.zeros((len(y), classes), dtype=np.float32)\n",
        "\n",
        "    all = [i for i in range(classes)]\n",
        "    for i in range(len(y)):\n",
        "      y_onehot[i][all.index(y[i])] = 1\n",
        "\n",
        "    return y_onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM0XiPhGbt5u"
      },
      "source": [
        "## Training and Validating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daJvkYmez-rO"
      },
      "source": [
        "### Define Model Saving Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQPDgrcIV4lw"
      },
      "source": [
        "CNN - 0 </br>\n",
        "vgg - 1 </br>\n",
        "basset - 2 </br>\n",
        "deapsea - 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2YtffBfe-Jd"
      },
      "source": [
        "save_model_time = '0'\n",
        "mkpath = 'model/model%s'% save_model_time\n",
        "# os.makedirs(mkpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49Lxjh_C0Emr"
      },
      "source": [
        "### Trainer for Categorical Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWtILhz40LVD"
      },
      "source": [
        "class TrainHelper():\n",
        "    '''\n",
        "    Helper class that makes it a bit easier and cleaner to define the training routine\n",
        "    \n",
        "    '''\n",
        "\n",
        "    def __init__(self,model,train_set,test_set,opts):\n",
        "      self.model = model  # neural net\n",
        "\n",
        "      # device agnostic code snippet\n",
        "      self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.model.to(self.device)\n",
        "\n",
        "      self.epochs = opts['epochs']\n",
        "      self.optimizer = torch.optim.Adam(model.parameters(), opts['lr']) # optimizer method for gradient descent\n",
        "      #self.optimizer = torch.optim.SGD(model.parameters(), opts['lr'])\n",
        "      if opts['loss_fxn'] == 'c':\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()                      # loss function\n",
        "      else:\n",
        "        self.criterion = torch.nn.BCEWithLogitsLoss()                    # loss function used in papers\n",
        "\n",
        "      self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                                      batch_size=opts['batch_size'],\n",
        "                                                      shuffle=True)\n",
        "      self.valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                                      batch_size=opts['batch_size'],\n",
        "                                                      shuffle=True)\n",
        "    def train(self):\n",
        "      self.model.train() # put model in training mode\n",
        "      for epoch in range(self.epochs):\n",
        "          self.tr_loss = []\n",
        "          for i, (data,labels) in tqdm_notebook(enumerate(self.train_loader),\n",
        "                                                  total = len(self.train_loader)):\n",
        "\n",
        "              data, labels = data.to(self.device),labels.to(self.device)\n",
        "              self.optimizer.zero_grad()  \n",
        "              outputs = self.model(data)\n",
        "\n",
        "              blabels = labels\n",
        "              if opts['loss_fxn'] == 'b':\n",
        "                blabels = torch.from_numpy(onehot(labels)).to(self.device)\n",
        "\n",
        "              loss = self.criterion(outputs, blabels)                  # loss function\n",
        "              loss.backward()           \n",
        "              self.optimizer.step()                  \n",
        "              self.tr_loss.append(loss.item())       \n",
        "          if (epoch+1) % 5 == 0 or epoch == 0: # save the model every _ epoch\n",
        "\n",
        "              torch.save(self.model, 'model/model{save_model_time}/net_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int((epoch+1)/5)))\n",
        "              torch.save(self.model.state_dict(), 'model/model{save_model_time}/net_params_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int((epoch+1)/5)))\n",
        "          \n",
        "          self.test(epoch) # run through the validation set\n",
        "\n",
        "    def test(self,epoch):\n",
        "            \n",
        "      self.model.eval()    # puts model in eval mode\n",
        "      self.test_loss = []\n",
        "      self.test_accuracy = []\n",
        "\n",
        "      for i, (data, labels) in enumerate(self.valid_loader):\n",
        "          \n",
        "          data, labels = data.to(self.device),labels.to(self.device)\n",
        "          # pass data through network\n",
        "          # turn off gradient calculation to speed up calcs and reduce memory\n",
        "          with torch.no_grad():\n",
        "              outputs = self.model(data)\n",
        "\n",
        "          # make our predictions and update our loss info\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "          blabels = labels\n",
        "          if opts['loss_fxn'] == 'b':\n",
        "              blabels = torch.from_numpy(onehot(labels)).to(self.device)\n",
        "\n",
        "          loss = self.criterion(outputs, blabels) \n",
        "\n",
        "          self.test_loss.append(loss.item())\n",
        "          # print(predicted)\n",
        "          # print(labels)\n",
        "          self.test_accuracy.append((predicted == labels).sum().item() / predicted.size(0))\n",
        "      \n",
        "      test_loss.append(np.mean(self.test_loss))\n",
        "      train_loss.append(np.mean(self.tr_loss))\n",
        "      bestmodel(self.model,save_model_time,np.mean(self.test_loss)) # find best model\n",
        "      print('epoch: {}, train loss: {}, test loss: {}, test accuracy: {}'.format( \n",
        "            epoch+1, np.mean(self.tr_loss), np.mean(self.test_loss), np.mean(self.test_accuracy)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YZlrZHZ0MMz"
      },
      "source": [
        "### Trainer for Continuous Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fEN2GunQxQl"
      },
      "source": [
        "class TrainHelper():\n",
        "    '''\n",
        "    Helper class that makes it a bit easier and cleaner to define the training routine\n",
        "    \n",
        "    '''\n",
        "\n",
        "    def __init__(self,model,train_set,test_set,opts):\n",
        "      self.model = model  # neural net\n",
        "\n",
        "      # device agnostic code snippet\n",
        "      self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.model.to(self.device)\n",
        "\n",
        "      self.epochs = opts['epochs']\n",
        "      self.optimizer = torch.optim.Adam(model.parameters(), opts['lr']) # optimizer method for gradient descent\n",
        "      #self.optimizer = torch.optim.SGD(model.parameters(), opts['lr'])\n",
        "      self.criterion = torch.nn.MSELoss()\n",
        "      self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                                      batch_size=opts['batch_size'],\n",
        "                                                      shuffle=True)\n",
        "      self.valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                                      batch_size=opts['batch_size'],\n",
        "                                                      shuffle=True)\n",
        "    def train(self):\n",
        "      self.model.train() # put model in training mode\n",
        "      for epoch in range(self.epochs):\n",
        "          self.tr_loss = []\n",
        "          for i, (data,labels) in tqdm_notebook(enumerate(self.train_loader),\n",
        "                                                  total = len(self.train_loader)):\n",
        "\n",
        "              data, labels = data.to(self.device),labels.to(self.device)\n",
        "              self.optimizer.zero_grad()  \n",
        "              outputs = self.model(data)\n",
        "              labels = labels.unsqueeze(1)\n",
        "              loss = self.criterion(outputs.float(), labels.float())\n",
        "              loss.backward()                        \n",
        "              self.optimizer.step()                  \n",
        "              self.tr_loss.append(loss.item())       \n",
        "          if epoch % 5 == 0: # save the model every _ epoch\n",
        "              torch.save(self.model, 'model/model{save_model_time}/net_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int(epoch/5)))\n",
        "              torch.save(self.model.state_dict(), 'model/model{save_model_time}/net_params_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int(epoch/5)))\n",
        "          \n",
        "          self.test(epoch) # run through the validation set\n",
        "\n",
        "    def test(self,epoch):\n",
        "            \n",
        "      self.model.eval()    # puts model in eval mode\n",
        "      self.test_loss = []\n",
        "      self.test_accuracy = []\n",
        "\n",
        "      for i, (data, labels) in enumerate(self.valid_loader):\n",
        "          \n",
        "          data, labels = data.to(self.device),labels.to(self.device)\n",
        "          # pass data through network\n",
        "          # turn off gradient calculation to speed up calcs and reduce memory\n",
        "          with torch.no_grad():\n",
        "              outputs = self.model(data)\n",
        "          # make our predictions and update our loss info\n",
        "          # _, predicted = torch.max(outputs.data, 1)\n",
        "          # predicted = []\n",
        "          # for o in outputs:\n",
        "          #   predicted.append(o[0])\n",
        "          labels = labels.unsqueeze(1)\n",
        "          loss = self.criterion(outputs, labels)\n",
        "          self.test_loss.append(loss.item())\n",
        "      \n",
        "      test_loss.append(np.mean(self.test_loss))\n",
        "      train_loss.append(np.mean(self.tr_loss))    \n",
        "      bestmodel(self.model,save_model_time,np.mean(self.test_loss)) # find best model\n",
        "      print('epoch: {}, train loss: {}, test loss: {}'.format( \n",
        "      epoch+1, np.mean(self.tr_loss), np.mean(self.test_loss)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ_9_cQRZmoI"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAkfdvpVbGJw"
      },
      "source": [
        "### Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2PIgCj_dYix"
      },
      "source": [
        "Choosing test_dataset or sub_dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYLBGckjVt_z"
      },
      "source": [
        "train_X, train_y = shuffle(train_X, train_y, random_state=0) \n",
        "train_X_sub = train_X[:2000]\n",
        "train_y_sub = train_y[:2000]\n",
        "sub_dataset = MyDataset(train_X_sub, train_y_sub)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBnwO857ZvOB"
      },
      "source": [
        "def test_result(model, datatype):\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
        "    if datatype == 'sub':\n",
        "      test_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=100, shuffle=True)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    test_accuracy = []\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      data, labels = data.to(device),labels.to(device)\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "          outputs = model(data)\n",
        "    # make our predictions and update our loss info\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      test_accuracy.append((predicted == labels).sum().item() / predicted.size(0))\n",
        "    if datatype == 'sub':\n",
        "      print('Training accuracy', np.mean(test_accuracy))\n",
        "    else:\n",
        "      print('Testing accuracy', np.mean(test_accuracy))\n",
        "    return np.mean(test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcH6n-08bJZv"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I-J36R9bMUt"
      },
      "source": [
        "def confusion(test_data, classifier):\n",
        "    M = np.zeros((classes,classes))\n",
        "    predict = []\n",
        "    label = []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      data, labels = data.to(device),labels.to(device)\n",
        "      label.extend(labels.tolist())\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "        outputs = classifier(data)\n",
        "    # make our predictions and update our loss info\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      predict.extend(predicted.tolist())\n",
        "\n",
        "    tmp = [i for i in range(classes)]\n",
        "    M = confusion_matrix(label, predict, labels = tmp)  \n",
        "    return M\n",
        "\n",
        "def visualize_confusion(M):\n",
        "    fig = plt.figure(figsize = (10, 10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    tmp = [i for i in range(classes)]\n",
        "    cm = ConfusionMatrixDisplay(M, display_labels = tmp);\n",
        "    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\n",
        "    # plt.xticks(rotation = 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd0S_FkWzxhp"
      },
      "source": [
        "### Get Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWjrGsEvz6wu"
      },
      "source": [
        "#### Categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftPtAaLtdUbo"
      },
      "source": [
        "def get_list_cat(model):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pred, true = [], []\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      data, labels = data.to(device),labels.to(device)\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "          outputs = model(data)\n",
        "    # make our predictions and update our loss info\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      pred.extend(predicted.tolist())\n",
        "      true.extend(labels.tolist())\n",
        "    return true, pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syQPyNFaz-Jx"
      },
      "source": [
        "#### Continuous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFN8jU9dihGA"
      },
      "source": [
        "def get_list_con(model):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pred, true = [], []\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      data, labels = data.to(device),labels.to(device)\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "          outputs = model(data)\n",
        "    # make our predictions and update our loss info\n",
        "      predicted = []\n",
        "      for o in outputs.tolist():\n",
        "        predicted.append(o[0])\n",
        "      pred.extend(predicted)\n",
        "      true.extend(labels.tolist())\n",
        "    return true, pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yrhP_CazCRz"
      },
      "source": [
        "### AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec3xfDAGzFEf"
      },
      "source": [
        "def getAUC(model):\n",
        "    labels, predicts = get_list_cat(model)\n",
        "    score = metrics.roc_auc_score(labels, predicts, average='weighted')\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_EGhJ2614GL"
      },
      "source": [
        "### Pearson R"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt2n5x1v2JBA"
      },
      "source": [
        "from scipy import stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRO9fDng16nQ"
      },
      "source": [
        "def getR(model):\n",
        "    labels, predicts = get_list_cat(model)\n",
        "    corr, _ = stats.pearsonr(labels, predicts)\n",
        "    return corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaJTngEEess_"
      },
      "source": [
        "### Plot Train Verse Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJyLKJ75ex2I"
      },
      "source": [
        " def pltacc(train_acc, test_acc, epoch):\n",
        "    epochs = [i for i in range(epoch+1)][::5][1:]\n",
        "    fig = plt.figure()\n",
        "    plt.plot(epochs, train_acc, 'g', label='Training accuracy')\n",
        "    plt.plot(epochs, test_acc, 'b', label='Testing accuracy')\n",
        "    plt.title('Training and Testing accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbh8YD10S5_R"
      },
      "source": [
        "### Plot Train Verse Test Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTXEE1nySuz0"
      },
      "source": [
        "def pltloss(train_loss, test_loss, epoch):\n",
        "    epochs = [i for i in range(epoch)]\n",
        "    fig = plt.figure()\n",
        "    plt.plot(epochs, train_loss, 'g', label='Training loss')\n",
        "    plt.plot(epochs, test_loss, 'b', label='Testing loss')\n",
        "    plt.title('Training and Testing Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQcFhIG4dGQw"
      },
      "source": [
        "### Plot Predicated Verse Label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxp62JohiPNU"
      },
      "source": [
        "#### Categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwNkhjYieTbN"
      },
      "source": [
        "def plotcomp(model):\n",
        "    labels, predicts = get_list_cat(model)\n",
        "    idx_list = [i for i in range(len(labels))]\n",
        "    idx_sele = random.sample(idx_list, 50)\n",
        "    fig = plt.figure()\n",
        "    label_sele, pred_sele = [], []\n",
        "    for i in idx_sele:\n",
        "      label_sele.append(labels[i])\n",
        "      pred_sele.append(predicts[i])\n",
        "    plt.scatter(pred_sele, label_sele, c='g', marker='x')\n",
        "    plt.title('Actual Values vs Predicated Values')\n",
        "    plt.xlabel('Predicated Values')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlim(0, max(pred_sele))\n",
        "    plt.ylim(0, max(label_sele))\n",
        "    # plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx4TGh2FiV47"
      },
      "source": [
        "#### Continuous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUFJt7rGiMeB"
      },
      "source": [
        "def plotcomp(model):\n",
        "    labels, predicts = get_list_con(model)\n",
        "    idx_list = [i for i in range(len(labels))]\n",
        "    idx_sele = random.sample(idx_list, 50)\n",
        "    fig = plt.figure()\n",
        "    label_sele, pred_sele = [], []\n",
        "    for i in idx_sele:\n",
        "      label_sele.append(labels[i])\n",
        "      pred_sele.append(predicts[i])\n",
        "    plt.scatter(pred_sele, label_sele, c='b', marker='+')\n",
        "    plt.title('Actual Values vs Predicated Values')\n",
        "    plt.xlabel('Predicated Values')\n",
        "    plt.ylabel('Actual Values')\n",
        "    plt.xlim(0, max(pred_sele))\n",
        "    plt.ylim(0, max(label_sele))\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wh8i5QjZig_"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erQbncE7cF5C"
      },
      "source": [
        "### BERT (not in use)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5nubGbD_ium"
      },
      "source": [
        "# class BertClassifier(nn.Module):\n",
        "#   def __init__(self, config):\n",
        "#     super(BertClassifier, self).__init__()\n",
        "    \n",
        "#     self.num_labels = config.num_labels\n",
        "#     # Pre-trained BERT model\n",
        "#     self.bert = BertModel(config)\n",
        "#     # Dropout to avoid overfitting\n",
        "#     self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "#     # A single layer classifier added on top of BERT to fine tune for regression\n",
        "#     self.predict = torch.nn.Linear(config.hidden_size, config.output)\n",
        "#     # Weight initialization \n",
        "#     torch.nn.init.xavier_normal_(self.classifier.weight)\n",
        "  \n",
        "#   def forward(self, input_ids, token_type_ids=None, attention_mask=None, position_ids=None, head_mask=None):\n",
        "#     # Forward pass through pre-trained BERT\n",
        "#     outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "#                           attention_mask=attention_mask, head_mask=head_mask)\n",
        "#     # Last layer output (Total 12 layers)\n",
        "#     pooled_output = outputs[-1]\n",
        "#     pooled_output = self.dropout(pooled_output) \n",
        "#     return self.classifier(pooled_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_lGX2QRbyvJ"
      },
      "source": [
        "### CNN Naive Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ6eU2J7RbrL"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        \"\"\"\n",
        "        init convolution and activation layers\n",
        "        Args:\n",
        "        x: (Nx1x2004)\n",
        "        class: \n",
        "\n",
        "        \"\"\"\n",
        "        super(CNN, self).__init__() \n",
        "        \n",
        "        self.conv1 = torch.nn.Conv1d(input_size[0], 32, 3)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv1d(32, 64, 3)\n",
        "        self.pool = torch.nn.MaxPool1d(4)\n",
        "        self.fc1 = torch.nn.Linear(2304, num_classes)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward function describes how input tensor is transformed to output tensor\n",
        "        Args:\n",
        "            \n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # print(x.size())\n",
        "        # assert 0\n",
        "        x = self.fc1(x)\n",
        "        x = self.sig(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4neNZyBb4vY"
      },
      "source": [
        "model = CNN(train_X.shape[1:], classes)\n",
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KT0_7pWb71P"
      },
      "source": [
        "opts = {\n",
        "    'lr': 5e-4,\n",
        "    'epochs': 50,\n",
        "    'batch_size': 100,\n",
        "    'loss_fxn': 'c'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wYrl72qb-Il"
      },
      "source": [
        "test_loss, train_loss = [], []\n",
        "CNNTrainer = TrainHelper(model = model,\n",
        "                      train_set = train_dataset,\n",
        "                      test_set = valid_dataset, opts = opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDPc0NiRcBwc"
      },
      "source": [
        "CNNTrainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUDkkSoCaeix"
      },
      "source": [
        "#### Check for Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnlnxvO9_UfV"
      },
      "source": [
        "test_result(model,'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7S6VuMyKFMe"
      },
      "source": [
        "Get accuracy:<br>'sub' -> subset of training dataset <br/>'test' -> test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUGA4DHGnYTY"
      },
      "source": [
        "train_acc, test_acc = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2qoeGkhm-zX"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  model.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  model.cuda()\n",
        "  train_acc.append(test_result(model, 'sub'))\n",
        "  test_acc.append(test_result(model, 'test'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C3qIYN2Pi-d"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  model.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  model.cuda()\n",
        "  print(getR(model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n69jsH5ICU9D"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTiteAq8CY9g"
      },
      "source": [
        "M = confusion(test_loader, model)\n",
        "visualize_confusion(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN9KbJSOn1g6"
      },
      "source": [
        "pltacc(train_acc, test_acc, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYf-aezVn3bf"
      },
      "source": [
        "pltloss(train_loss, test_loss, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq0PfG45b6Ik"
      },
      "source": [
        "### DeepSEA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_ooH_Sl_u_j"
      },
      "source": [
        "class DeepSEA(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(DeepSEA, self).__init__()\n",
        "        self.Conv1 = nn.Conv1d(in_channels=input_size[0], out_channels=320, kernel_size=8)\n",
        "        self.Conv2 = nn.Conv1d(in_channels=320, out_channels=480, kernel_size=8)\n",
        "        self.Conv3 = nn.Conv1d(in_channels=480, out_channels=960, kernel_size=8)\n",
        "        self.Maxpool = nn.MaxPool1d(kernel_size=6, stride=6)\n",
        "        self.Drop1 = nn.Dropout(p=0.2)\n",
        "        self.Drop2 = nn.Dropout(p=0.5)\n",
        "        self.Linear1 = nn.Linear(7680, 2000)\n",
        "        self.Linear2 = nn.Linear(2000, num_classes)\n",
        "        self.Linear3 = nn.Linear(1000, 2)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = x.unsqueeze(1)\n",
        "        x = self.Conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.Maxpool(x)\n",
        "        x = self.Drop1(x)\n",
        "        x = self.Conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.Maxpool(x)\n",
        "        x = self.Drop1(x)\n",
        "        x = self.Conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.Drop2(x)\n",
        "        # print(x.size())\n",
        "        x = torch.flatten(x, 1)\n",
        "        # x = x.view(-1, 5760)\n",
        "        # print(x.size())\n",
        "        # assert 0\n",
        "        x = self.Linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.Linear2(x)\n",
        "        # x = self.relu(x)\n",
        "        # x = self.Linear3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Ahx1OTEZix"
      },
      "source": [
        "opts = {\n",
        "    'lr': 1e-3,\n",
        "    'epochs': 1,\n",
        "    'batch_size': 100,\n",
        "    'loss_fxn': 'c'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjGig49pEsPu"
      },
      "source": [
        "deepsea = DeepSEA(train_X.shape[1:], classes)\n",
        "deepsea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIeYpB0tEguS"
      },
      "source": [
        "test_loss, train_loss = [], []\n",
        "deepseaTrainer = TrainHelper(model = deepsea,\n",
        "                      train_set = train_dataset,\n",
        "                      test_set = valid_dataset,opts = opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmnCKHMHEH0N"
      },
      "source": [
        "deepseaTrainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_rYK8uOatyI"
      },
      "source": [
        "#### Check for Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlXUJeAMJDvD"
      },
      "source": [
        "test_result(deepsea, 'sub')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy_j8DBV71er"
      },
      "source": [
        "train_acc, test_acc = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P0iHLCmfsKV"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  deepsea.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  deepsea.cuda()\n",
        "  train_acc.append(test_result(deepsea, 'sub'))\n",
        "  test_acc.append(test_result(deepsea, 'test'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcgxudVkbxbT"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvG1qGiNbt3U"
      },
      "source": [
        "M = confusion(test_loader, deepsea)\n",
        "visualize_confusion(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1QzBbvoAyut"
      },
      "source": [
        "pltacc(train_acc, test_acc, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cLAS_7tDdLY"
      },
      "source": [
        "pltloss(train_loss, test_loss, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkC9I-8Tsg1O"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  deepsea.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  deepsea.cuda()\n",
        "  pred_sele = []\n",
        "  label_sele = []\n",
        "  plotcomp(deepsea)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cJzBRgFYObq"
      },
      "source": [
        "### Basset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p487gZXaYIDQ"
      },
      "source": [
        "class Basset(nn.Module):\n",
        "    def __init__(self, input_size, num_class):\n",
        "        super(Basset, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size[0], out_channels=300, kernel_size=19)\n",
        "        self.batch1 = nn.BatchNorm1d(num_features=300)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=300, out_channels=200, kernel_size=11)\n",
        "        self.batch2 = nn.BatchNorm1d(num_features=200)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=4)\n",
        "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=200, kernel_size=7)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=2000, out_features=1000)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=1000, out_features=1000)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.fc3 = nn.Linear(in_features=1000, out_features=num_class)\n",
        "        self.fc4 = nn.Linear(in_features=164, out_features=2)\n",
        "        self.sig3 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #output = inputs.unsqueeze(1)\n",
        "        output = self.conv1(inputs)\n",
        "        output = self.batch1(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.pool1(output)\n",
        "\n",
        "        output = self.conv2(output)\n",
        "        output = self.batch2(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.pool2(output)\n",
        "\n",
        "\n",
        "        output = self.conv3(output)\n",
        "        output = self.batch2(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.pool2(output)\n",
        "\n",
        "        output = torch.flatten(output, 1)\n",
        "        \n",
        "        output = self.fc1(output)\n",
        "        output = self.relu4(output)\n",
        "        output = self.dropout1(output)\n",
        "\n",
        "        output = self.fc2(output)\n",
        "        output = self.relu5(output)\n",
        "        output = self.dropout2(output)\n",
        "\n",
        "        output = self.fc3(output)\n",
        "        output = self.sig3(output)\n",
        "        # output = self.fc4(output)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLWBzegcYabF"
      },
      "source": [
        "opts = {\n",
        "    'lr': 1e-4,\n",
        "    'epochs': 5,\n",
        "    'batch_size': 100,\n",
        "    'loss_fxn': 'c'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGPyL0kld_bi"
      },
      "source": [
        "basset = Basset(train_X.shape[1:], 2)\n",
        "basset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2uS1hFY0Az"
      },
      "source": [
        "test_loss, train_loss = [], []\n",
        "bassetTrainer = TrainHelper(model = basset,\n",
        "                      train_set = train_dataset,\n",
        "                      test_set = valid_dataset,opts = opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nw7QKDYY-aJ"
      },
      "source": [
        "bassetTrainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW5OL5Z-bo_M"
      },
      "source": [
        "#### Check for Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mx2NMf7hOHd"
      },
      "source": [
        "test_result(basset, 'sub')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC0O3Cg-jdTE"
      },
      "source": [
        "train_acc, test_acc = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IALtj_DXPr8Y"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  basset.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  basset.cuda()\n",
        "  train_acc.append(test_result(basset, 'sub'))\n",
        "  test_acc.append(test_result(basset, 'test'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_96IbkYk-s4q"
      },
      "source": [
        "Pearson R"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcSOnZXB-onY"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  basset.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  basset.cuda()\n",
        "  print(getR(basset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktouXAu8-x0U"
      },
      "source": [
        "AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYwsbjAg-04W"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  basset.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  basset.cuda()\n",
        "  print(getAUC(basset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTdKIz3VcHpE"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UXtHaHG963Y"
      },
      "source": [
        "Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4THHmXO2cMOY"
      },
      "source": [
        "M = confusion(test_loader, basset)\n",
        "visualize_confusion(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4knqY9HT-BZ0"
      },
      "source": [
        "Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZcZoSlilNCI"
      },
      "source": [
        "pltacc(train_acc, test_acc, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98tiNCJf-MJR"
      },
      "source": [
        "Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-_l5WFdCJe8"
      },
      "source": [
        "pltloss(train_loss, test_loss, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk7GY4DZ-VmX"
      },
      "source": [
        "Others"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMVqTHS3INPf"
      },
      "source": [
        "plotcomp(basset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBrMSYrA33LP"
      },
      "source": [
        "### VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMUTBSNh6PUl"
      },
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.conv1_1 = nn.Conv1d(input_size[0], 64, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv2_1 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv3_1 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv4_1 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv5_1 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        # max pooling (kernel_size, stride)\n",
        "        self.pool = nn.MaxPool1d(2, 2)\n",
        "\n",
        "        # fully conected layers\n",
        "        self.fc6 = nn.Linear(512*18, 1000)\n",
        "        self.fc7 = nn.Linear(1000, 100)\n",
        "        self.fc8 = nn.Linear(100, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv1_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.conv2_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.conv3_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3_3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.conv4_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv4_2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv4_3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.relu(self.conv5_1(x))\n",
        "        x = self.relu(self.conv5_2(x))\n",
        "        x = self.relu(self.conv5_3(x))\n",
        "        x = self.pool(x)\n",
        "        # print(x.size())\n",
        "        x = torch.flatten(x, 1)\n",
        "        # print(x.size())\n",
        "        # assert 0\n",
        "        x = self.fc6(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc7(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc8(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxstk-3DWDdI"
      },
      "source": [
        "opts = {\n",
        "    'lr': 1e-4,\n",
        "    'epochs': 5,\n",
        "    'batch_size': 100,\n",
        "    'loss_fxn': 'c'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SilcyfLHWJSn"
      },
      "source": [
        "vgg = VGG16(train_X.shape[1:], classes)\n",
        "vgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EbLr7fHWRf_"
      },
      "source": [
        "test_loss, train_loss = [], []\n",
        "vggTrainer = TrainHelper(model = vgg,\n",
        "                      train_set = train_dataset,\n",
        "                      test_set = valid_dataset,opts = opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVgvxMKbWdBH"
      },
      "source": [
        "vggTrainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn965DMQDPRC"
      },
      "source": [
        "#### Check for Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBACzT6qDT_p"
      },
      "source": [
        "test_result(vgg, 'sub')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exdoJlab4TKN"
      },
      "source": [
        "train_acc, test_acc = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLriS97t4URF"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  vgg.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  vgg.cuda()\n",
        "  train_acc.append(test_result(vgg, 'sub'))\n",
        "  test_acc.append(test_result(vgg, 'test'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zloexfeUFtZc"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7KPjQirDhDc"
      },
      "source": [
        "M = confusion(test_loader, vgg)\n",
        "visualize_confusion(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcn3VRpeFx16"
      },
      "source": [
        "pltacc(train_acc, test_acc, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwxQ8WXI4pKE"
      },
      "source": [
        "pltloss(train_loss, test_loss, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoPW870uMUNo"
      },
      "source": [
        "### Linear Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rkOLmXPMWHq"
      },
      "source": [
        "# Define Single Layer Perceptron network\n",
        "class SLP(nn.Module):\n",
        "    def __init__(self, in_features, classes):\n",
        "        super(SLP, self).__init__()\n",
        "        # model variables\n",
        "        self.layer1 = nn.Linear(in_features,classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # linear operation\n",
        "        x = torch.flatten(x, 1)\n",
        "        y_pred = self.layer1(x)\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bZZjYsiMeIe"
      },
      "source": [
        "slp = SLP((train_X.shape[1:][0]*train_X.shape[1:][1], classes))\n",
        "slp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbfZH5jKMlcg"
      },
      "source": [
        "opts = {\n",
        "    'lr': 1e-4,\n",
        "    'epochs': 5,\n",
        "    'batch_size': 100,\n",
        "    'loss_fxn': 'c'\n",
        "}\n",
        "test_loss, train_loss = [], []\n",
        "SLPTrainer = TrainHelper(model = slp,\n",
        "                      train_set = train_dataset,\n",
        "                      test_set = valid_dataset,opts = opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Geu1_1bWMrtn"
      },
      "source": [
        "SLPTrainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0jczr-NNKkA"
      },
      "source": [
        "test_result(slp, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8SWOsOROm7K"
      },
      "source": [
        "M = confusion(test_loader, slp)\n",
        "visualize_confusion(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0V0BEAoZALU"
      },
      "source": [
        "### LeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a14sPoepY-FV"
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size[0], out_channels=6, kernel_size=5) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=4)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=120, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=8280, out_features=84)\n",
        "        self.fc2 = nn.Linear(in_features=84, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        # print(x.size())\n",
        "        # assert 0\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POKwBlVna_c-"
      },
      "source": [
        "lenet = LeNet(train_X.shape[1:], classes)\n",
        "lenet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d43mux4kbEFq"
      },
      "source": [
        "opts = {\n",
        "    'lr': 1e-3,\n",
        "    'epochs': 25,\n",
        "    'batch_size': 100,\n",
        "    'loss_fxn': 'c'\n",
        "}\n",
        "test_loss, train_loss = [], []\n",
        "LeNetTrainer = TrainHelper(model = lenet,\n",
        "                      train_set = train_dataset,\n",
        "                      test_set = valid_dataset, opts = opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvHQ6KJmbNoT"
      },
      "source": [
        "LeNetTrainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG6BJzAJeGOH"
      },
      "source": [
        "#### Check for Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5atmcKpeJHz"
      },
      "source": [
        "train_acc, test_acc = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnwJxbayeOhz"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  lenet.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  lenet.cuda()\n",
        "  train_acc.append(test_result(lenet, 'sub'))\n",
        "  test_acc.append(test_result(lenet, 'test'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2s4pXPDe5Pp"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xepd7W8be7Uu"
      },
      "source": [
        "pltacc(train_acc, test_acc, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzqBMwpae7pe"
      },
      "source": [
        "pltloss(train_loss, test_loss, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B5wnXY7fWSH"
      },
      "source": [
        "M = confusion(test_loader, lenet)\n",
        "visualize_confusion(M)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}