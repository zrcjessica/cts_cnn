{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_cell.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GooSNT2NSxiH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1eK5ikwUhN"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muX6m69LwBmP"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import pandas as pd \n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import pickle\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htrnzR9LwI9W"
      },
      "source": [
        "from tqdm import tqdm_notebook, trange\n",
        "\n",
        "def seed_everything(seed = 42): \n",
        "  random.seed(seed) \n",
        "  os.environ['PYTHONHASHSEED'] = str(seed) \n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed) \n",
        "  torch.cuda.manual_seed(seed) \n",
        "  torch.backends.cudnn.deterministic = True\n",
        "# For reproducible results\n",
        "seed_everything()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfm0tvzrwLJZ"
      },
      "source": [
        "import matplotlib as mpl\n",
        "mpl.style.use('seaborn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNcqeWAewNV2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W7HRhQTwQBN"
      },
      "source": [
        "%cd /content/gdrive/My Drive/seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2h30bTfwTGe"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAG2up1RwXc_"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.data = X\n",
        "        self.target = Y\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        s = []\n",
        "        for i in range(types):\n",
        "          s.append(self.target[index][i])\n",
        "        \n",
        "        return x, s\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs1z73vZxObw"
      },
      "source": [
        "### Reading Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJHnkl7f-76Q"
      },
      "source": [
        "%ls data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf-dP5Tgw0za"
      },
      "source": [
        "npzfile = np.load('data/mic_sst_comb_cap.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpnPCKOpw8Vv"
      },
      "source": [
        "X, Y = npzfile['arr_0'], npzfile['arr_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeX992Hqe67H"
      },
      "source": [
        "X, Y = shuffle(X, Y, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUpv1EFI9nA1"
      },
      "source": [
        "types = len(Y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGWoGbnbw-Gd"
      },
      "source": [
        "classes = max(max(Y[:,0]), max(Y[:,1])) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GooSNT2NSxiH"
      },
      "source": [
        "#### Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rViRYQSZxNVV"
      },
      "source": [
        "test_size = Counter(Y[:,0])[0]*0.2\n",
        "cap1, cap2 = test_size, test_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsi__19pxtG0"
      },
      "source": [
        "testX, testY, idx_L = [], [], []\n",
        "for idx, y in enumerate(Y):\n",
        "  if y == 0 and cap1 > 0:\n",
        "    testY.append(y)\n",
        "    testX.append(X[idx])\n",
        "    idx_L.append(idx)\n",
        "    cap1 -= 1\n",
        "  if y == 1 and cap2 > 0:\n",
        "    testY.append(y)\n",
        "    testX.append(X[idx])\n",
        "    idx_L.append(idx)\n",
        "    cap2 -= 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3LKqrlWDeqP"
      },
      "source": [
        "validX, validY = [],[]\n",
        "for idx, y in enumerate(Y):\n",
        "  if idx in idx_L:\n",
        "    continue \n",
        "  if y == 0 and cap1 > 0:\n",
        "    validY.append(y)\n",
        "    validX.append(X[idx])\n",
        "    idx_L.append(idx)\n",
        "    cap1 -= 1\n",
        "  if y == 1 and cap2 > 0:\n",
        "    validY.append(y)\n",
        "    validX.append(X[idx])\n",
        "    idx_L.append(idx)\n",
        "    cap2 -= 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM8AQcMqDzp4"
      },
      "source": [
        "trainX, trainY = [] []\n",
        "for idx, y in enumerate(Y):\n",
        "  if idx in idx_L:\n",
        "    continue \n",
        "  trainX.append(X[idx])\n",
        "  trainY.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WXJnEjmfKjI"
      },
      "source": [
        "### Divide Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb79o5GQS2yp"
      },
      "source": [
        "testX = X[int(len(Y)*0.8):]\n",
        "testY = Y[int(len(Y)*0.8):]\n",
        "validX = X[int(len(Y)*0.6):int(len(Y)*0.8)]\n",
        "validY = Y[int(len(Y)*0.6):int(len(Y)*0.8)]\n",
        "trainX = X[:int(len(Y)*0.6)]\n",
        "trainY = Y[:int(len(Y)*0.6)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EybqD2gPxGwy"
      },
      "source": [
        "trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
        "validX, validY = shuffle(validX, validY, random_state=0)\n",
        "testX, testY = shuffle(testX, testY, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCEFeyXjEiHi"
      },
      "source": [
        "### Convert to Torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8xfSOKDEkAI"
      },
      "source": [
        "train_X = torch.from_numpy(trainX)\n",
        "train_y = torch.from_numpy(trainY)\n",
        "valid_X  = torch.from_numpy(validX)\n",
        "valid_y = torch.from_numpy(validY)\n",
        "test_X = torch.from_numpy(testX)\n",
        "test_y = torch.from_numpy(testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7pzJe9mEkxV"
      },
      "source": [
        "train_dataset = MyDataset(train_X, train_y)\n",
        "valid_dataset = MyDataset(valid_X, valid_y)\n",
        "test_dataset = MyDataset(test_X, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrV7Cb-iEmYt"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKuRmIVKEoV2"
      },
      "source": [
        "def bestmodel(model_name,save_model_time,valid_loss):\n",
        "    bestloss = 10000\n",
        "    if valid_loss < bestloss :\n",
        "        bestloss = valid_loss\n",
        "        torch.save(model_name, 'model/model{save_model_time}/bestmodel.pkl'.format(save_model_time=save_model_time))\n",
        "        torch.save(model_name.state_dict(), 'model/model{save_model_time}/net_params_bestmodel.pkl'.format(save_model_time=save_model_time))\n",
        "    return True  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWF0P_qJEqk-"
      },
      "source": [
        "def onehot(y):\n",
        "    y_onehot = np.zeros((len(y), classes), dtype=np.float32)\n",
        "\n",
        "    all = [i for i in range(classes)]\n",
        "    for i in range(len(y)):\n",
        "      y_onehot[i][all.index(y[i])] = 1\n",
        "\n",
        "    return y_onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAW0PEJPEsk3"
      },
      "source": [
        "## Training and Validating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mAywbLfEuLk"
      },
      "source": [
        "save_model_time = '0'\n",
        "mkpath = 'model/model%s'% save_model_time\n",
        "# os.makedirs(mkpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RziAGGVEv8Y"
      },
      "source": [
        "class TrainHelper():\n",
        "    '''\n",
        "    Helper class that makes it a bit easier and cleaner to define the training routine\n",
        "    \n",
        "    '''\n",
        "\n",
        "    def __init__(self,model,train_set,test_set,opts):\n",
        "      self.model = model  # neural net\n",
        "\n",
        "      # device agnostic code snippet\n",
        "      self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.model.to(self.device)\n",
        "\n",
        "      self.epochs = opts['epochs']\n",
        "      self.optimizer = torch.optim.Adam(model.parameters(), opts['lr']) # optimizer method for gradient descent\n",
        "      #self.optimizer = torch.optim.SGD(model.parameters(), opts['lr'])\n",
        "      if opts['loss_fxn'] == 'c':\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()                      # loss function\n",
        "      else:\n",
        "        self.criterion = torch.nn.BCEWithLogitsLoss()                    # loss function used in papers\n",
        "\n",
        "      self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                                      batch_size=opts['batch_size'],\n",
        "                                                      shuffle=True)\n",
        "      self.valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                                      batch_size=opts['batch_size'],\n",
        "                                                      shuffle=True)\n",
        "    def train(self):\n",
        "      self.model.train() # put model in training mode\n",
        "      for epoch in range(self.epochs):\n",
        "          self.tr_loss = []\n",
        "          for i, (data,labels) in tqdm_notebook(enumerate(self.train_loader),\n",
        "                                                  total = len(self.train_loader)):\n",
        "              label_list = []\n",
        "              for i in range(len(labels)):\n",
        "                label_list.append(labels[i].to(self.device))\n",
        "              data = data.to(self.device)\n",
        "              self.optimizer.zero_grad()  \n",
        "              outputs = self.model(data)\n",
        "\n",
        "              b_list = []\n",
        "              for i in range(len(label_list)):\n",
        "                b_list.append(label_list[i])\n",
        "              if opts['loss_fxn'] == 'b':\n",
        "                for i in range(len(label_list)):\n",
        "                  b_list[i] = torch.from_numpy(onehot(labels[i])).to(self.device)\n",
        "\n",
        "              loss = 0  # define loss\n",
        "              for i in range(len(outputs)):\n",
        "                loss += self.criterion(outputs[i], b_list[i])\n",
        "   \n",
        "              loss.backward()           \n",
        "              self.optimizer.step()                  \n",
        "              self.tr_loss.append(loss.item())       \n",
        "          if (epoch+1) % 5 == 0 or epoch == 0: # save the model every _ epoch\n",
        "\n",
        "              torch.save(self.model, 'model/model{save_model_time}/net_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int((epoch+1)/5)))\n",
        "              torch.save(self.model.state_dict(), 'model/model{save_model_time}/net_params_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int((epoch+1)/5)))\n",
        "          \n",
        "          self.test(epoch) # run through the validation set\n",
        "\n",
        "    def test(self,epoch):\n",
        "            \n",
        "      self.model.eval()    # puts model in eval mode\n",
        "      self.test_loss = []\n",
        "      self.test_accuracy_L = [[] for _ in range(types)]\n",
        "\n",
        "      for i, (data, labels) in enumerate(self.valid_loader):\n",
        "          \n",
        "          label_list = []\n",
        "          for i in range(len(labels)):\n",
        "              label_list.append(labels[i].to(self.device))\n",
        "          data = data.to(self.device)\n",
        "          # pass data through network\n",
        "          # turn off gradient calculation to speed up calcs and reduce memory\n",
        "          with torch.no_grad():\n",
        "              outputs = self.model(data)\n",
        "\n",
        "          # make our predictions and update our loss info\n",
        "          pred_list = []\n",
        "          for i in range(len(outputs)):\n",
        "            _, predicted = torch.max(outputs[i].data, 1)\n",
        "            pred_list.append(predicted)\n",
        "\n",
        "          b_list = []\n",
        "          for i in range(len(label_list)):\n",
        "            b_list.append(label_list[i])\n",
        "          if opts['loss_fxn'] == 'b':\n",
        "            for i in range(len(label_list)):\n",
        "              b_list[i] = torch.from_numpy(onehot(labels[i])).to(self.device)\n",
        "\n",
        "          loss = 0  # define loss\n",
        "          for i in range(len(outputs)):\n",
        "            loss += self.criterion(outputs[i], b_list[i])\n",
        "\n",
        "          self.test_loss.append(loss.item())\n",
        "\n",
        "          for i in range(len(pred_list)):\n",
        "            self.test_accuracy_L[i].append((pred_list[i] == label_list[i]).sum().item() / pred_list[i].size(0))\n",
        "      \n",
        "      test_loss.append(np.mean(self.test_loss))\n",
        "      train_loss.append(np.mean(self.tr_loss))\n",
        "      av = [np.mean(self.test_accuracy_L[i]) for i in range(types)]\n",
        "      bestmodel(self.model,save_model_time,np.mean(self.test_loss)) # find best model\n",
        "      print('epoch: {}, train loss: {}, test loss: {}, test accuracy: {}'.format( \n",
        "            epoch+1, np.mean(self.tr_loss), np.mean(self.test_loss), av))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQcDHhvQKucE"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwaM4eFzKwpc"
      },
      "source": [
        "train_X, train_y = shuffle(train_X, train_y, random_state=0) \n",
        "train_X_sub = train_X[:2000]\n",
        "train_y_sub = train_y[:2000]\n",
        "sub_dataset = MyDataset(train_X_sub, train_y_sub)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYPCsLHbKxr8"
      },
      "source": [
        "def test_result(model, datatype):\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
        "    if datatype == 'sub':\n",
        "      test_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=100, shuffle=True)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    test_accuracy_L = [[] for _ in range(types)]\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      label_list = []\n",
        "      for i in range(len(labels)):\n",
        "          label_list.append(labels[i].to(device))\n",
        "      data = data.to(device)\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "          outputs = model(data)\n",
        "    # make our predictions and update our loss info\n",
        "      pred_list = []\n",
        "      for i in range(len(outputs)):\n",
        "        _, predicted = torch.max(outputs[i].data, 1)\n",
        "        pred_list.append(predicted)\n",
        "      for i in range(len(pred_list)):\n",
        "        test_accuracy_L[i].append((pred_list[i] == label_list[i]).sum().item() / pred_list[i].size(0))\n",
        "    # if datatype == 'sub':\n",
        "    #   print('Training accuracy for cell 1: {}, Training accuracy for cell 2: {}'.format( \n",
        "    #         np.mean(test_accuracy1), np.mean(test_accuracy2)))\n",
        "    # else:\n",
        "    #   print('Testing accuracy for cell 1: {}, Testing accuracy for cell 2: {}'.format(\n",
        "    #        np.mean(test_accuracy1), np.mean(test_accuracy2)))\n",
        "    return [np.mean(test_accuracy_L[i]) for i in range(types)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRcvgEiiLwwo"
      },
      "source": [
        "def pltloss(train_loss, test_loss, epoch):\n",
        "    epochs = [i for i in range(epoch)]\n",
        "    fig = plt.figure()\n",
        "    plt.plot(epochs, train_loss, 'g', label='Training loss')\n",
        "    plt.plot(epochs, test_loss, 'b', label='Testing loss')\n",
        "    plt.title('Training and Testing Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T50L4gOBnHOD"
      },
      "source": [
        "Need better graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmvjZPS3picP"
      },
      "source": [
        " def pltacc(tr_acc_1, ts_acc_1, tr_acc_2, ts_acc_2, tr_acc_3, ts_acc_3, tr_acc_4, ts_acc_4, epoch):\n",
        "    epochs = [i for i in range(epoch+1)][::5][1:]\n",
        "    fig = plt.figure()\n",
        "    plt.plot(epochs, tr_acc_1, 'g', linestyle='dashed', label='Training Accuracy for cell 1')\n",
        "    plt.plot(epochs, ts_acc_1, 'g', label='Testing Accuracy for cell 1')\n",
        "    plt.plot(epochs, tr_acc_2, 'b', linestyle='dashed', label='Training Accuracy for cell 2')\n",
        "    plt.plot(epochs, ts_acc_2, 'b', label='Testing Accuracy for cell 2')\n",
        "    plt.plot(epochs, tr_acc_3, 'r', linestyle='dashed', label='Training Accuracy for cell 3')\n",
        "    plt.plot(epochs, ts_acc_3, 'r', label='Testing Accuracy for cell 3')\n",
        "    plt.plot(epochs, tr_acc_4, 'y', linestyle='dashed', label='Training Accuracy for cell 4')\n",
        "    plt.plot(epochs, ts_acc_4, 'y', label='Testing Accuracy for cell 4')\n",
        "\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW0sgtWdL1pw"
      },
      "source": [
        " def pltacc(tr_acc_1, ts_acc_1, tr_acc_2, ts_acc_2, epoch):\n",
        "    epochs = [i for i in range(epoch+1)][::5][1:]\n",
        "    fig = plt.figure()\n",
        "    plt.plot(epochs, tr_acc_1, 'g', linestyle='dashed', label='Training Accuracy for cell 1')\n",
        "    plt.plot(epochs, ts_acc_1, 'b', label='Testing Accuracy for cell 1')\n",
        "    plt.plot(epochs, tr_acc_2, 'r', linestyle='dashed', label='Training Accuracy for cell 2')\n",
        "    plt.plot(epochs, ts_acc_2, 'y', label='Testing Accuracy for cell 2')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgew5-DOMHdQ"
      },
      "source": [
        "def confusion(test_data, classifier, num):\n",
        "    M = np.zeros((classes,classes))\n",
        "    pred, label = [], []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      data, labels_i = data.to(device),labels[num].to(device)\n",
        "      label.extend(labels_i.tolist())\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "        outputs = classifier(data)\n",
        "    # make our predictions and update our loss info\n",
        "      _, predicted = torch.max(outputs[num].data, 1)\n",
        "      pred.extend(predicted.tolist())\n",
        "\n",
        "    tmp = [i for i in range(classes)]\n",
        "    M = confusion_matrix(label, pred, labels = tmp)\n",
        "\n",
        "    return M\n",
        "\n",
        "def visualize_confusion(M):\n",
        "    fig = plt.figure(figsize = (5, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    tmp = [i for i in range(classes)]\n",
        "    cm = ConfusionMatrixDisplay(M, display_labels = tmp);\n",
        "    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8ovkhqkMLci"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsMhtXGKMM5p"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        \"\"\"\n",
        "        init convolution and activation layers\n",
        "        Args:\n",
        "        x: (Nx4x601)\n",
        "        class: \n",
        "\n",
        "        \"\"\"\n",
        "        super(CNN, self).__init__() \n",
        "        \n",
        "        self.conv1 = torch.nn.Conv1d(input_size[0], 32, 3)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv1d(32, 64, 3)\n",
        "        self.pool = torch.nn.MaxPool1d(4)\n",
        "        self.fc1 = torch.nn.Linear(2304, num_classes)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward function describes how input tensor is transformed to output tensor\n",
        "        Args:\n",
        "            \n",
        "        \"\"\"\n",
        "        # shared layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        output = []\n",
        "        for i in range(types):\n",
        "          tmp = self.fc1(x)\n",
        "          tmp = self.sig(tmp)\n",
        "          output.append(tmp)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLMtT3nxNpZw"
      },
      "source": [
        "cnn = CNN(train_X.shape[1:], classes)\n",
        "cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0Xl_mfvNrmo"
      },
      "source": [
        "opts = {\n",
        "    'lr': 5e-4,\n",
        "    'epochs': 50,\n",
        "    'batch_size': 100,\n",
        "    'loss_fxn': 'c'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFZZs8BwNtri"
      },
      "source": [
        "test_loss, train_loss = [], []\n",
        "CNNTrainer = TrainHelper(model = cnn,\n",
        "                      train_set = train_dataset,\n",
        "                      test_set = valid_dataset, opts = opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9xzMaQNNvvG"
      },
      "source": [
        "CNNTrainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hciawmsnj1D8"
      },
      "source": [
        "#### Check for Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmYF7iiEN0Dm"
      },
      "source": [
        "test_result(cnn,'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZry3YIkkGbn"
      },
      "source": [
        "train_acc1, train_acc2, train_acc3, train_acc4, test_acc1, test_acc2, test_acc3, test_acc4 = [], [], [], [], [], [], [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB-RltL5j7QQ"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  cnn.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  cnn.cuda()\n",
        "  tmp_train = test_result(cnn, 'sub')\n",
        "  tmp_test = test_result(cnn, 'test')\n",
        "  train_acc1.append(tmp_train[0])\n",
        "  train_acc2.append(tmp_train[1])\n",
        "  train_acc3.append(tmp_train[2])\n",
        "  train_acc4.append(tmp_train[3])\n",
        "  test_acc1.append(tmp_test[0])\n",
        "  test_acc2.append(tmp_test[1])\n",
        "  test_acc3.append(tmp_test[2])\n",
        "  test_acc4.append(tmp_test[3])\n",
        "  print(tmp_train)\n",
        "  print(tmp_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFoCEpULqUGf"
      },
      "source": [
        "max(test_acc4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkkvbAV4p-9o"
      },
      "source": [
        "cnn.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_10.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEc4gsTdWWOZ"
      },
      "source": [
        "M1 = confusion(test_loader, cnn, 0)\n",
        "M2 = confusion(test_loader, cnn, 1)\n",
        "M3 = confusion(test_loader, cnn, 2)\n",
        "M4 = confusion(test_loader, cnn, 3)\n",
        "visualize_confusion(M1)\n",
        "visualize_confusion(M2)\n",
        "visualize_confusion(M3)\n",
        "visualize_confusion(M4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJaCqZ0SYLS4"
      },
      "source": [
        "pltloss(train_loss, test_loss, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnt2o1Sdo61u"
      },
      "source": [
        "pltacc(train_acc1, test_acc1, train_acc2, test_acc2, train_acc3, test_acc3, train_acc4, test_acc4, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt8PhXnjpG5V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}