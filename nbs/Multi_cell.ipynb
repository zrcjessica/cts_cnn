{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Multi_cell.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1eK5ikwUhN"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muX6m69LwBmP"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import pandas as pd \n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import pickle\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import f1_score, recall_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import PrecisionRecallDisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWREJa4M7bDK"
      },
      "source": [
        "!pip install focal-loss-torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htrnzR9LwI9W"
      },
      "source": [
        "from tqdm import tqdm_notebook, trange\n",
        "\n",
        "def seed_everything(seed = 42): \n",
        "  random.seed(seed) \n",
        "  os.environ['PYTHONHASHSEED'] = str(seed) \n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed) \n",
        "  torch.cuda.manual_seed(seed) \n",
        "  torch.backends.cudnn.deterministic = True\n",
        "# For reproducible results\n",
        "seed_everything()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfm0tvzrwLJZ"
      },
      "source": [
        "import matplotlib as mpl\n",
        "mpl.style.use('seaborn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNcqeWAewNV2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W7HRhQTwQBN"
      },
      "source": [
        "%cd /content/gdrive/My Drive/seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2h30bTfwTGe"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAG2up1RwXc_"
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.data = X\n",
        "        self.target = Y\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        s = []\n",
        "        for i in range(types):\n",
        "          s.append(self.target[index][i])\n",
        "        \n",
        "        return x, s\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs1z73vZxObw"
      },
      "source": [
        "### Reading Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJHnkl7f-76Q"
      },
      "source": [
        "%ls data/MACS2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf-dP5Tgw0za"
      },
      "source": [
        "npzfile = np.load('data/MACS2/merge_c2_neg.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpnPCKOpw8Vv"
      },
      "source": [
        "X, Y = npzfile['arr_0'], npzfile['arr_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeX992Hqe67H"
      },
      "source": [
        "X, Y = shuffle(X, Y, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUpv1EFI9nA1"
      },
      "source": [
        "types = len(Y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma9k9AR3h-fj"
      },
      "source": [
        "types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGWoGbnbw-Gd"
      },
      "source": [
        "classes = max(max(Y[:,0]), max(Y[:,1])) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WXJnEjmfKjI"
      },
      "source": [
        "### Divide Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb79o5GQS2yp"
      },
      "source": [
        "testX = X[int(len(Y)*0.8):]\n",
        "testY = Y[int(len(Y)*0.8):]\n",
        "validX = X[int(len(Y)*0.6):int(len(Y)*0.8)]\n",
        "validY = Y[int(len(Y)*0.6):int(len(Y)*0.8)]\n",
        "trainX = X[:int(len(Y)*0.6)]\n",
        "trainY = Y[:int(len(Y)*0.6)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EybqD2gPxGwy"
      },
      "source": [
        "trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
        "validX, validY = shuffle(validX, validY, random_state=0)\n",
        "testX, testY = shuffle(testX, testY, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYUfEoy3JImX"
      },
      "source": [
        "Data Distrubution for Each Cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5ODxauYVixS"
      },
      "source": [
        "for i in range(types):\n",
        "  print(Counter(Y[:,i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCEFeyXjEiHi"
      },
      "source": [
        "### Convert to Torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8xfSOKDEkAI"
      },
      "source": [
        "train_X = torch.from_numpy(trainX)\n",
        "train_y = torch.from_numpy(trainY)\n",
        "valid_X  = torch.from_numpy(validX)\n",
        "valid_y = torch.from_numpy(validY)\n",
        "test_X = torch.from_numpy(testX)\n",
        "test_y = torch.from_numpy(testY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7pzJe9mEkxV"
      },
      "source": [
        "train_dataset = MyDataset(train_X, train_y)\n",
        "valid_dataset = MyDataset(valid_X, valid_y)\n",
        "test_dataset = MyDataset(test_X, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrV7Cb-iEmYt"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKuRmIVKEoV2"
      },
      "source": [
        "def bestmodel(model_name,save_model_time,valid_loss):\n",
        "    bestloss = 10000\n",
        "    if valid_loss < bestloss :\n",
        "        bestloss = valid_loss\n",
        "        torch.save(model_name, 'model/model{save_model_time}/bestmodel.pkl'.format(save_model_time=save_model_time))\n",
        "        torch.save(model_name.state_dict(), 'model/model{save_model_time}/net_params_bestmodel.pkl'.format(save_model_time=save_model_time))\n",
        "    return True  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWF0P_qJEqk-"
      },
      "source": [
        "def onehot(y):\n",
        "    y_onehot = np.zeros((len(y), classes), dtype=np.float32)\n",
        "\n",
        "    all = [i for i in range(classes)]\n",
        "    for i in range(len(y)):\n",
        "      y_onehot[i][all.index(y[i])] = 1\n",
        "\n",
        "    return y_onehot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAW0PEJPEsk3"
      },
      "source": [
        "## Training and Validating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mAywbLfEuLk"
      },
      "source": [
        "save_model_time = '0'\n",
        "mkpath = 'model/model%s'% save_model_time\n",
        "# os.makedirs(mkpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXRNpXbM7uwg"
      },
      "source": [
        "from focal_loss.focal_loss import FocalLoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RziAGGVEv8Y"
      },
      "source": [
        "class TrainHelper():\n",
        "    '''\n",
        "    Helper class that makes it a bit easier and cleaner to define the training routine\n",
        "    \n",
        "    '''\n",
        "\n",
        "    def __init__(self,model,train_set,test_set,opts):\n",
        "      self.model = model  # neural net\n",
        "\n",
        "      # device agnostic code snippet\n",
        "      self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.model.to(self.device)\n",
        "\n",
        "      self.epochs = opts['epochs']\n",
        "      if opts['opt'] == 'Adam':\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), opts['lr']) # optimizer method for gradient descent\n",
        "      else:\n",
        "        self.optimizer = torch.optim.SGD(model.parameters(), opts['lr'])\n",
        "      if opts['loss_fxn'] == 'c':\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()                      # loss function\n",
        "      elif opts['loss_fxn'] == 'b':\n",
        "        self.criterion = torch.nn.BCEWithLogitsLoss()                    # loss function used in papers\n",
        "      elif opts['loss_fxn'] == 'f':\n",
        "        self.criterion = FocalLoss(alpha=0.25, gamma=2)\n",
        "\n",
        "      self.train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                                      batch_size=opts['batch_size'],\n",
        "                                                      shuffle=True)\n",
        "      self.valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                                      batch_size=opts['batch_size'],\n",
        "                                                      shuffle=True)\n",
        "    def train(self):\n",
        "      self.model.train() # put model in training mode\n",
        "      for epoch in range(self.epochs):\n",
        "          self.tr_loss = []\n",
        "          for i, (data,labels) in tqdm_notebook(enumerate(self.train_loader),\n",
        "                                                  total = len(self.train_loader)):\n",
        "              label_list = []\n",
        "              for i in range(len(labels)):\n",
        "                label_list.append(labels[i].to(self.device))\n",
        "              data = data.to(self.device)\n",
        "              self.optimizer.zero_grad()  \n",
        "              outputs = self.model(data)\n",
        "\n",
        "              b_list = []\n",
        "              for i in range(len(label_list)):\n",
        "                b_list.append(label_list[i])\n",
        "              if opts['loss_fxn'] != 'c':\n",
        "                for i in range(len(label_list)):\n",
        "                  b_list[i] = torch.from_numpy(onehot(labels[i])).to(self.device)\n",
        "\n",
        "              loss = 0  # define loss\n",
        "              for i in range(len(outputs)):\n",
        "                loss += self.criterion(outputs[i], b_list[i])\n",
        "   \n",
        "              loss.backward()           \n",
        "              self.optimizer.step()                  \n",
        "              self.tr_loss.append(loss.item())       \n",
        "          if (epoch+1) % 5 == 0 or epoch == 0: # save the model every _ epoch\n",
        "\n",
        "              torch.save(self.model, 'model/model{save_model_time}/net_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int((epoch+1)/5)))\n",
        "              torch.save(self.model.state_dict(), 'model/model{save_model_time}/net_params_{epoch}.pkl'.format(save_model_time=save_model_time,epoch=int((epoch+1)/5)))\n",
        "          \n",
        "          self.test(epoch) # run through the validation set\n",
        "\n",
        "    def test(self,epoch):\n",
        "            \n",
        "      self.model.eval()    # puts model in eval mode\n",
        "      self.test_loss = []\n",
        "      self.test_accuracy_L = [[] for _ in range(types)]\n",
        "\n",
        "      for i, (data, labels) in enumerate(self.valid_loader):\n",
        "          \n",
        "          label_list = []\n",
        "          for i in range(len(labels)):\n",
        "              label_list.append(labels[i].to(self.device))\n",
        "          data = data.to(self.device)\n",
        "          # pass data through network\n",
        "          # turn off gradient calculation to speed up calcs and reduce memory\n",
        "          with torch.no_grad():\n",
        "              outputs = self.model(data)\n",
        "\n",
        "          # make our predictions and update our loss info\n",
        "          pred_list = []\n",
        "          for i in range(len(outputs)):\n",
        "            _, predicted = torch.max(outputs[i].data, 1)\n",
        "            pred_list.append(predicted)\n",
        "\n",
        "          b_list = []\n",
        "          for i in range(len(label_list)):\n",
        "            b_list.append(label_list[i])\n",
        "          if opts['loss_fxn'] != 'c':\n",
        "            for i in range(len(label_list)):\n",
        "              b_list[i] = torch.from_numpy(onehot(labels[i])).to(self.device)\n",
        "\n",
        "          loss = 0  # define loss\n",
        "          for i in range(len(outputs)):\n",
        "            loss += self.criterion(outputs[i], b_list[i])\n",
        "\n",
        "          self.test_loss.append(loss.item())\n",
        "\n",
        "          for i in range(len(pred_list)):\n",
        "            self.test_accuracy_L[i].append((pred_list[i] == label_list[i]).sum().item() / pred_list[i].size(0))\n",
        "      \n",
        "      test_loss.append(np.mean(self.test_loss))\n",
        "      train_loss.append(np.mean(self.tr_loss))\n",
        "      av = [np.mean(self.test_accuracy_L[i]) for i in range(types)]\n",
        "      bestmodel(self.model,save_model_time,np.mean(self.test_loss)) # find best model\n",
        "      print('epoch: {}, train loss: {}, test loss: {}, test accuracy: {}'.format( \n",
        "            epoch+1, np.mean(self.tr_loss), np.mean(self.test_loss), av))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQcDHhvQKucE"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwaM4eFzKwpc"
      },
      "source": [
        "train_X, train_y = shuffle(train_X, train_y, random_state=0) \n",
        "train_X_sub = train_X[:2000]\n",
        "train_y_sub = train_y[:2000]\n",
        "sub_dataset = MyDataset(train_X_sub, train_y_sub)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYPCsLHbKxr8"
      },
      "source": [
        "def test_result(model, datatype):\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
        "    if datatype == 'sub':\n",
        "      test_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=100, shuffle=True)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    test_accuracy_L = [[] for _ in range(types)]\n",
        "    f1 = [0 for _ in range(types)]\n",
        "    recall = [0 for _ in range(types)]\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      label_list = []\n",
        "      for i in range(len(labels)):\n",
        "          label_list.append(labels[i].to(device))\n",
        "      data = data.to(device)\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "          outputs = model(data)\n",
        "    # make our predictions and update our loss info\n",
        "      pred_list = []\n",
        "      for i in range(len(outputs)):\n",
        "        _, predicted = torch.max(outputs[i].data, 1)\n",
        "        pred_list.append(predicted)\n",
        "      for i in range(len(pred_list)):\n",
        "        test_accuracy_L[i].append((pred_list[i] == label_list[i]).sum().item() / pred_list[i].size(0))\n",
        "        f1[i] = f1_score(label_list[i].tolist(), pred_list[i].tolist(), average=None)\n",
        "        recall[i] = recall_score(label_list[i].tolist(), pred_list[i].tolist(), average=None)\n",
        "    return [np.mean(test_accuracy_L[i]) for i in range(types)], f1, recall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRcvgEiiLwwo"
      },
      "source": [
        "def pltloss(train_loss, test_loss, epoch):\n",
        "    epochs = [i for i in range(epoch)]\n",
        "    fig = plt.figure()\n",
        "    plt.plot(epochs, train_loss, 'g', label='Training loss')\n",
        "    plt.plot(epochs, test_loss, 'b', label='Testing loss')\n",
        "    plt.title('Training and Testing Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T50L4gOBnHOD"
      },
      "source": [
        "Need better graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmvjZPS3picP"
      },
      "source": [
        "def pltacc_test(acc_list, epoch):\n",
        "    epochs = [i for i in range(epoch+1)][::5][1:]\n",
        "    fig = plt.figure()\n",
        "    num = acc_list[0]\n",
        "    acc_list = np.array(acc_list)\n",
        "    colors = ['g', 'b', 'r', 'c', 'm', 'y', 'k', 'w']\n",
        "    for i in range(len(num)):\n",
        "      tmp = 'Testing Accuracy for cell'+str(i+1)\n",
        "      plt.plot(epochs, acc_list[:,i], colors[i], label=tmp)\n",
        "\n",
        "    plt.title('Testing Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XRG7o7P57zw"
      },
      "source": [
        " def pltacc_train(acc_list, epoch):\n",
        "    epochs = [i for i in range(epoch+1)][::5][1:]\n",
        "    fig = plt.figure()\n",
        "    num = acc_list[0]\n",
        "    acc_list = np.array(acc_list)\n",
        "    colors = ['g', 'b', 'r', 'c', 'm', 'y', 'k', 'w']\n",
        "    for i in range(len(num)):\n",
        "      tmp = 'Training Accuracy for cell'+str(i+1)\n",
        "      plt.plot(epochs, acc_list[:,i], colors[i], linestyle='dashed', label=tmp)\n",
        "\n",
        "    plt.title('Training Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWUW67idJTG6"
      },
      "source": [
        "Compare Accuracy Side by Side "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW0sgtWdL1pw"
      },
      "source": [
        " def pltacc(tr_acc_1, ts_acc_1, tr_acc_2, ts_acc_2, epoch):\n",
        "    epochs = [i for i in range(epoch+1)][::5][1:]\n",
        "    fig = plt.figure()\n",
        "    plt.plot(epochs, tr_acc_1, 'g', linestyle='dashed', label='Training Accuracy for cell 1')\n",
        "    plt.plot(epochs, ts_acc_1, 'g', label='Testing Accuracy for cell 1')\n",
        "    plt.plot(epochs, tr_acc_2, 'b', linestyle='dashed', label='Training Accuracy for cell 2')\n",
        "    plt.plot(epochs, ts_acc_2, 'b', label='Testing Accuracy for cell 2')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgew5-DOMHdQ"
      },
      "source": [
        "def confusion(test_data, classifier, num):\n",
        "    M = np.zeros((classes,classes))\n",
        "    pred, label = [], []\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      data, labels_i = data.to(device),labels[num].to(device)\n",
        "      label.extend(labels_i.tolist())\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "        outputs = classifier(data)\n",
        "    # make our predictions and update our loss info\n",
        "      _, predicted = torch.max(outputs[num].data, 1)\n",
        "      pred.extend(predicted.tolist())\n",
        "\n",
        "    tmp = [i for i in range(classes)]\n",
        "    M = confusion_matrix(label, pred, labels = tmp)\n",
        "    print(M.diagonal()/M.sum(axis=1))\n",
        "    # TN = M[0][0]\n",
        "    # FN = M[1][0]\n",
        "    # TP = M[1][1]\n",
        "    # FP = M[0][1]\n",
        "    # TPR = TP/(TP+FN)\n",
        "    # FPR = FP/(FP+TN)\n",
        "    # print('TPR', TPR)\n",
        "    # print('FPR', FPR)\n",
        "    # print('ACC', (TP+TN)/(TP+FP+FN+TN))\n",
        "    # AUC = metrics.roc_auc_score(label, pred, labels = tmp)\n",
        "    # print('AUC', AUC)\n",
        "    # print('\\n')\n",
        "\n",
        "    return M\n",
        "\n",
        "def visualize_confusion(M):\n",
        "    fig = plt.figure(figsize = (5, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    tmp = [i for i in range(classes)]\n",
        "    cm = ConfusionMatrixDisplay(M, display_labels = tmp);\n",
        "    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlM1hXb5NIlj"
      },
      "source": [
        "def getAUC(model, num):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pred, label = [], []\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      label_list = labels[num].to(device)\n",
        "      label.extend(label_list.tolist())\n",
        "      data = data.to(device)\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "          outputs = model(data)\n",
        "    # make our predictions and update our loss info\n",
        "      _, predicted = torch.max(outputs[num].data, 1)\n",
        "      pred.extend(predicted.tolist())\n",
        "    AUC = metrics.roc_auc_score(label, pred, labels=[0,1])\n",
        "    return AUC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbbQZ4Kg6XA2"
      },
      "source": [
        "def getAUPRC(model, num):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pred, label = [], []\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      label_list = labels[num].to(device)\n",
        "      label.extend(label_list.tolist())\n",
        "      data = data.to(device)\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "          outputs = model(data)\n",
        "    # make our predictions and update our loss info\n",
        "      _, predicted = torch.max(outputs[num].data, 1)\n",
        "      pred.extend(predicted.tolist())\n",
        "    auprc = average_precision_score(label, pred)\n",
        "    return auprc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Es5CKXO-h5X"
      },
      "source": [
        "def plotAUPRC(model, num):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    pred, label = [], []\n",
        "    for i, (data, labels) in enumerate(test_loader):\n",
        "      label_list = labels[num].to(device)\n",
        "      label.extend(label_list.tolist())\n",
        "      data = data.to(device)\n",
        "    # pass data through network\n",
        "    # turn off gradient calculation to speed up calcs and reduce memory\n",
        "      with torch.no_grad():\n",
        "          outputs = model(data)\n",
        "    # make our predictions and update our loss info\n",
        "      _, predicted = torch.max(outputs[num].data, 1)\n",
        "      pred.extend(predicted.tolist())\n",
        "    precision, recall, thresholds = precision_recall_curve(label, pred)\n",
        "    disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
        "    disp.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8ovkhqkMLci"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm8AcBMYbAL4"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsMhtXGKMM5p"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        \"\"\"\n",
        "        init convolution and activation layers\n",
        "        Args:\n",
        "        x: (Nx4x601)\n",
        "        class: \n",
        "\n",
        "        \"\"\"\n",
        "        super(CNN, self).__init__() \n",
        "        \n",
        "        self.conv1 = torch.nn.Conv1d(input_size[0], 32, 3)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.conv2 = torch.nn.Conv1d(32, 64, 3)\n",
        "        self.conv3 = torch.nn.Conv1d(64, 64, 3, dilation=1)\n",
        "        self.pool = torch.nn.MaxPool1d(4)\n",
        "        self.fc1 = torch.nn.Linear(2304, num_classes)\n",
        "        self.fc2 = torch.nn.Linear(512, num_classes)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward function describes how input tensor is transformed to output tensor\n",
        "        Args:\n",
        "            \n",
        "        \"\"\"\n",
        "        # shared layers\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "        output = []\n",
        "        # for i in range(2):\n",
        "        #   tmp = self.conv3(x)\n",
        "        #   tmp = self.relu(tmp)\n",
        "        #   tmp = self.pool(tmp)\n",
        "        #   tmp = torch.flatten(tmp, 1)\n",
        "        #   tmp = self.fc2(tmp)\n",
        "        #   tmp = self.sig(tmp)\n",
        "        #   output.append(tmp)\n",
        "        for i in range(types):\n",
        "          tmp = self.fc1(x)\n",
        "          tmp = self.sig(tmp)\n",
        "          output.append(tmp)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLMtT3nxNpZw"
      },
      "source": [
        "cnn = CNN(train_X.shape[1:], classes)\n",
        "cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0Xl_mfvNrmo"
      },
      "source": [
        "opts = {\n",
        "    'lr': 1e-4,\n",
        "    'epochs': 50,\n",
        "    'batch_size': 100,\n",
        "    'loss_fxn': 'b',\n",
        "    'opt': 'Adam'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFZZs8BwNtri"
      },
      "source": [
        "test_loss, train_loss = [], []\n",
        "CNNTrainer = TrainHelper(model = cnn,\n",
        "                      train_set = train_dataset,\n",
        "                      test_set = valid_dataset, opts = opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9xzMaQNNvvG"
      },
      "source": [
        "CNNTrainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hciawmsnj1D8"
      },
      "source": [
        "#### Check for Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZry3YIkkGbn"
      },
      "source": [
        "train_acc, tes_acc, tr_f1, tr_recall, ts_f1, ts_recall = [], [], [], [], [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB-RltL5j7QQ"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  cnn.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  cnn.cuda()\n",
        "  tmp_train, tmp_f1, tmp_r = test_result(cnn, 'sub')\n",
        "  tr_f1.append(tmp_f1)\n",
        "  tr_recall.append(tmp_r)\n",
        "  tmp_test, tmp_f1, tmp_r = test_result(cnn, 'test')\n",
        "  ts_f1.append(tmp_f1)\n",
        "  ts_recall.append(tmp_r)\n",
        "  train_acc.append(tmp_train)\n",
        "  tes_acc.append(tmp_test)\n",
        "  print(tmp_train)\n",
        "  print(tmp_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZLzfDA9bP0I"
      },
      "source": [
        "cnn.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_bestmodel.pkl'))\n",
        "for i in range(types):\n",
        "  print(getAUC(cnn, i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi3VKCQkhIS_"
      },
      "source": [
        "for i in range(types):\n",
        "  M = confusion(test_loader, cnn, i)\n",
        "  visualize_confusion(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJaCqZ0SYLS4"
      },
      "source": [
        "pltloss(train_loss, test_loss, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnt2o1Sdo61u"
      },
      "source": [
        "pltacc_test(tes_acc, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCB-iLLL6cQD"
      },
      "source": [
        "pltacc_train(train_acc, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqKuF8LskPHK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaRyTLRCbCkm"
      },
      "source": [
        "### Basset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fDoqsm0bDnE"
      },
      "source": [
        "class Basset(nn.Module):\n",
        "    def __init__(self, input_size, num_class):\n",
        "        super(Basset, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size[0], out_channels=300, kernel_size=19)\n",
        "        self.batch1 = nn.BatchNorm1d(num_features=300)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=300, out_channels=200, kernel_size=11)\n",
        "        self.batch2 = nn.BatchNorm1d(num_features=200)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=4)\n",
        "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=200, kernel_size=7)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=2000, out_features=1000)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=1000, out_features=1000)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.fc3 = nn.Linear(in_features=1000, out_features=num_class)\n",
        "        self.fc4 = nn.Linear(in_features=164, out_features=2)\n",
        "        self.sig3 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        #output = inputs.unsqueeze(1)\n",
        "        output = self.conv1(inputs)\n",
        "        output = self.batch1(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.pool1(output)\n",
        "\n",
        "        output = self.conv2(output)\n",
        "        output = self.batch2(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.pool2(output)\n",
        "\n",
        "\n",
        "        output = self.conv3(output)\n",
        "        output = self.batch2(output)\n",
        "        output = self.relu(output)\n",
        "        output = self.pool2(output)\n",
        "\n",
        "        output = torch.flatten(output, 1)\n",
        "        output = self.fc1(output)\n",
        "        output = self.relu4(output)\n",
        "        output = self.dropout1(output)\n",
        "\n",
        "        output = self.fc2(output)\n",
        "        output = self.relu5(output)\n",
        "        output = self.dropout2(output)\n",
        "\n",
        "        x = []\n",
        "        for i in range(types):\n",
        "          tmp = self.fc3(output)\n",
        "          tmp = self.sig3(tmp)\n",
        "          x.append(tmp)\n",
        "\n",
        "        # output = self.fc3(output)\n",
        "        # output = self.sig3(output)\n",
        "        # output = self.fc4(output)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po9jp72YbZPx"
      },
      "source": [
        "basset = Basset(train_X.shape[1:], classes)\n",
        "basset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q3L2NPxbSZI"
      },
      "source": [
        "opts = {\n",
        "    'lr': 1e-4,\n",
        "    'epochs': 25,\n",
        "    'batch_size': 100,\n",
        "    'loss_fxn': 'c',\n",
        "    'opt': 'SGD'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7ioTjXabVJS"
      },
      "source": [
        "test_loss, train_loss = [], []\n",
        "BassetTrainer = TrainHelper(model = basset,\n",
        "                      train_set = train_dataset,\n",
        "                      test_set = valid_dataset, opts = opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69TBVH2sbiJx"
      },
      "source": [
        "BassetTrainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4k8obSUNZak"
      },
      "source": [
        "#### Check for Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pUkXCHad-99"
      },
      "source": [
        "train_acc, tes_acc, tr_f1, tr_recall, ts_f1, ts_recall = [], [], [], [], [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhIid_M7IxRS"
      },
      "source": [
        "for num in range(opts['epochs']//5):\n",
        "  basset.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_'+str(num)+'.pkl'))\n",
        "  basset.cuda()\n",
        "  tmp_train, tmp_f1, tmp_r = test_result(basset, 'sub')\n",
        "  tr_f1.append(tmp_f1)\n",
        "  tr_recall.append(tmp_r)\n",
        "  tmp_test, tmp_f1, tmp_r = test_result(basset, 'test')\n",
        "  ts_f1.append(tmp_f1)\n",
        "  ts_recall.append(tmp_r)\n",
        "  train_acc.append(tmp_train)\n",
        "  tes_acc.append(tmp_test)\n",
        "  print(tmp_train)\n",
        "  print(tmp_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwVBjPRI2KtJ"
      },
      "source": [
        "ts_f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyJbvUqYI04E"
      },
      "source": [
        "pltloss(train_loss, test_loss, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbELXaroI4GN"
      },
      "source": [
        "pltacc_train(train_acc, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ2vxB1fI8e8"
      },
      "source": [
        "pltacc_test(tes_acc, opts['epochs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxgnruLeMc1-"
      },
      "source": [
        "basset.load_state_dict(torch.load('model/model'+save_model_time+'/net_params_1.pkl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLw__y4bbqH4"
      },
      "source": [
        "for i in range(types):\n",
        "  M = confusion(test_loader, basset, i)\n",
        "  visualize_confusion(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxK7-hgmJAx6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSu3mKWuqbQY"
      },
      "source": [
        "### LeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtM8VeGbqdtx"
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(LeNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size[0], out_channels=6, kernel_size=5) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=4)\n",
        "        self.conv3 = nn.Conv1d(in_channels=16, out_channels=120, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=8280, out_features=84)\n",
        "        self.fc2 = nn.Linear(in_features=84, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        # print(x.size())\n",
        "        # assert 0\n",
        "        output = []\n",
        "        for i in range(types):\n",
        "          tmp = self.fc1(x)\n",
        "          tmp = self.fc2(tmp)\n",
        "          output.append(tmp)\n",
        "\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMgRSLp7qgUh"
      },
      "source": [
        "lenet = LeNet(train_X.shape[1:], classes)\n",
        "lenet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxsjUJJ3qhVO"
      },
      "source": [
        "opts = {\n",
        "    'lr': 1e-4,\n",
        "    'epochs': 50,\n",
        "    'batch_size': 100,\n",
        "    'loss_fxn': 'c',\n",
        "    'opt': 'Adam'\n",
        "}\n",
        "test_loss, train_loss = [], []\n",
        "LeNetTrainer = TrainHelper(model = lenet,\n",
        "                      train_set = train_dataset,\n",
        "                      test_set = valid_dataset, opts = opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bk0OoaVqj7P"
      },
      "source": [
        "LeNetTrainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu2J_k0krVps"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}